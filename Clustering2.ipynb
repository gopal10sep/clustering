{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib qt \n",
    "\n",
    "import xlrd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import re\n",
    "import pandas as pd\n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    "from operator import itemgetter\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import manifold\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "import bokeh.plotting as bp\n",
    "from bokeh.models import HoverTool, BoxSelectTool\n",
    "from bokeh.plotting import figure, show, output_file\n",
    "import nltk\n",
    "import gensim\n",
    "import string\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import re\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from collections import defaultdict\n",
    "import math\n",
    "from scipy.spatial import distance\n",
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Open the workbook\n",
    "xl_workbook = xlrd.open_workbook(\"VR_AR_DATASET_USPTO.xlsx\") \n",
    "\n",
    "# Grabbing the Required Sheet by index \n",
    "xl_sheet = xl_workbook.sheet_by_index(0)\n",
    "\n",
    "#Initialising the Lists Required\n",
    "\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "output_file(\"test.html\")\n",
    "\n",
    "stemmer = nltk.SnowballStemmer(\"english\")\n",
    "\n",
    "topics = []\n",
    "currTopicList = []\n",
    "overallTopicList = []\n",
    "input_data = []\n",
    "\n",
    "def tokenize(text):\n",
    "    lemmatized_words = []\n",
    "    text = text.lower()\n",
    "    text = text.strip(\"\\n\")\n",
    "    text = re.sub(r'\\\\n',' ',text)\n",
    "    text = re.sub(r'[^\\w\\s]',' ',text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens_pos = pos_tag(tokens)\n",
    "    count = 0\n",
    "    for token in tokens:\n",
    "        pos = tokens_pos[count]\n",
    "        pos = get_wordnet_pos(pos[1])\n",
    "        if pos != '':\n",
    "            lemma = lemmatiser.lemmatize(token, pos)\n",
    "        else:\n",
    "            lemma = lemmatiser.lemmatize(token)\n",
    "        lemmatized_words.append(lemma)\n",
    "        count+=1\n",
    "    return lemmatized_words\n",
    "\n",
    "\n",
    "num_cols = xl_sheet.ncols   #Number of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for row_idx in range(2, xl_sheet.nrows): \n",
    "#for row_idx in range(2, 10):   # Iterate through rows\n",
    "\n",
    "    # for col_idx in range(1, 2):  \n",
    "    #     cell_obj = xl_sheet.cell(row_idx, col_idx)  # Get cell object by row, col\n",
    "    #     var1 = str(cell_obj)[7:-1]\n",
    " \n",
    "    col_idx = 1\n",
    "    doc1 = str(xl_sheet.cell(row_idx, col_idx))[7:-1]\n",
    "    col_idx = 2\n",
    "    doc2 = str(xl_sheet.cell(row_idx, col_idx))[7:-1]\n",
    "    col_idx = 3\n",
    "    doc3 = str(xl_sheet.cell(row_idx, col_idx))[7:-1]\n",
    "    col_idx = 4\n",
    "    doc4 = str(xl_sheet.cell(row_idx, col_idx))[7:-1]\n",
    "    col_idx == 5\n",
    "    doc5 = str(xl_sheet.cell(row_idx, col_idx))[7:-1]\n",
    "    col_idx == 6\n",
    "    doc6 = str(xl_sheet.cell(row_idx, col_idx))[7:-1]\n",
    "    col_idx = 20\n",
    "    doc7 = str(xl_sheet.cell(row_idx, col_idx))[7:-1]\n",
    "    col_idx = 38\n",
    "    doc8 = str(xl_sheet.cell(row_idx, col_idx))[7:-1]\n",
    "    col_idx =65\n",
    "    doc9 = str(xl_sheet.cell(row_idx, col_idx))[7:-1]\n",
    "    col_idx = 69\n",
    "    doc10 = str(xl_sheet.cell(row_idx, col_idx))[7:-1]\n",
    "    col_idx =72\n",
    "    doc11 = str(xl_sheet.cell(row_idx, col_idx))[7:-1]\n",
    "    col_idx =74\n",
    "    doc12 = str(xl_sheet.cell(row_idx, col_idx))[7:-1]\n",
    "    col_idx = 78\n",
    "    doc13 = str(xl_sheet.cell(row_idx, col_idx))[7:-1]\n",
    "    col_idx =79\n",
    "    doc14 = str(xl_sheet.cell(row_idx, col_idx))[7:-1]\n",
    "    col_idx =81\n",
    "    doc15 = str(xl_sheet.cell(row_idx, col_idx))[7:-1]\n",
    "    col_idx = 82\n",
    "    doc16 = str(xl_sheet.cell(row_idx, col_idx))[7:-1]\n",
    "    doc1 = doc1 + doc2 + doc3 + doc4 + doc5 + doc6 + doc7 + doc8 + doc9 + doc10+ doc11 + doc12 + doc13 + doc14 + doc15 + doc16\n",
    "\n",
    "    doc_complete = [doc1]\n",
    "\n",
    "\n",
    "    stop = set(stopwords.words('english'))\n",
    "    exclude = set(string.punctuation)\n",
    "    lemma = WordNetLemmatizer()\n",
    "\n",
    "    def clean(doc):\n",
    "        stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "        punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "        normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "        return normalized\n",
    "\n",
    "    doc_clean = [clean(doc).split() for doc in doc_complete]       \n",
    "\n",
    "\n",
    "    # Creating the term dictionary of our courpus, where every unique term is assigned an index. \n",
    "    dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "    # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "\n",
    "    # Creating the object for LDA model using gensim library\n",
    "    Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "    # Running and Trainign LDA model on the document term matrix.\n",
    "    ldamodel = Lda(doc_term_matrix, num_topics=1, id2word = dictionary, passes=10)\n",
    "\n",
    "    #Results\n",
    "    #curr_topic= str(ldamodel.print_topics(num_topics=3, num_words=3)[0])[12:-2]\n",
    "    curr_topic= str(ldamodel.print_topics(num_topics=1, num_words=5)[0])\n",
    "\n",
    "    #print(curr_topic)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    currTopicList =re.findall(r'\"([^\"]*)\"', curr_topic)\n",
    "    N = len(currTopicList)\n",
    "\n",
    "\n",
    "    for i in range(0, N): \n",
    "        overallTopicList.append(currTopicList[i])\n",
    "\n",
    "    topics.append(currTopicList)\n",
    "    #print currTopicList\n",
    "    #abc = currTopicList[0]+' '+currTopicList[1]+' '+currTopicList[2]+' '+currTopicList[3]+' '+currTopicList[4]+' '+currTopicList[5]+' '+currTopicList[6]+' '+currTopicList[7]+' '+currTopicList[8]+' '+currTopicList[9]\n",
    "    abc = currTopicList[0]+' '+currTopicList[1]+' '+currTopicList[2]+' '+currTopicList[3]+' '+currTopicList[4]\n",
    "    \n",
    "    abc = abc.strip(\"\\n\")\n",
    "    input_data.append(abc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This block plots the histogram between the topic terms and the quantity of patents in those topics.\n",
    "\n",
    "dictTopicList = {x:overallTopicList.count(x) for x in overallTopicList}\n",
    "dictTopicList = {k: v for k, v in dictTopicList.iteritems() if v >3}\n",
    "\n",
    "#print dictTopicList\n",
    "import operator\n",
    "sorted_x = sorted(dictTopicList.items(), key=operator.itemgetter(1),reverse=True)\n",
    "i = 0\n",
    "freq=[]\n",
    "terms=[]\n",
    "while ( i < len(sorted_x)):\n",
    "    freq.append(float(sorted_x[i][1]))\n",
    "    terms.append(sorted_x[i][0])\n",
    "    # print (type(sorted_x[i][0]))\n",
    "    # print (type(sorted_x[i][1]))\n",
    "    i = i +1\n",
    "\n",
    "\n",
    "#Plotting the Graph\n",
    "N = len(sorted_x)\n",
    "ind = np.arange(N)      #The x locations for the groups\n",
    "width = 0.35            #The width of the bars\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(ind, freq, width, color='r')\n",
    "#rects2 = ax.bar(ind + width, patent_citation_count, width, color='y')\n",
    "\n",
    "# #Add some text for labels, title and axes ticks\n",
    "\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Number of times occured')\n",
    "ax.set_xticks(ind)\n",
    "ax.set_xticklabels(terms, rotation='vertical')\n",
    "abc = \"Total number of topics\" + str(N)\n",
    "abc2 = \"Total number of patent analysed\" + str(xl_sheet.nrows)\n",
    "\n",
    "#Average no of topics per patent analysed\n",
    "\n",
    "# i = 0\n",
    "# abc3 = 0\n",
    "# while (i < N):\n",
    "# \tabc3 = abc3 + sorted_x[i][1]\n",
    "# \ti = i +1 \n",
    "\n",
    "\n",
    "# abc4 = \"Average no of topics per patent analysed\" + str(abc3) \n",
    "# abc5= abc3/xl_sheet.nrows\n",
    "\n",
    "# print (abc)\n",
    "# print (abc2)\n",
    "# print (abc5)\n",
    "#Total no of patents analysed\n",
    "#abc2 = \"Total number of topics\" + str(xl_sheet.nrows)\n",
    "ax.annotate(abc, xy=(0.9,0.9),xycoords='axes fraction',\n",
    "             fontsize=14)\n",
    "ax.annotate(abc2, xy=(0.5,0.5),xycoords='axes fraction',\n",
    "             fontsize=14)\n",
    "ax.annotate(abc5, xy=(0.1,0.1),xycoords='axes fraction',\n",
    "             fontsize=14)\n",
    "#ax.legend((rects1), ('Topic Modelled'))\n",
    "\n",
    "#Attach a text label above each bar displaying its height\n",
    "def autolabel(rects):    \n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        if height != 0 :\n",
    "            ax.text(rect.get_x() + rect.get_width()/2., 1.05*height,'%d' % int(height),ha='center', va='bottom')\n",
    "autolabel(rects1)\n",
    "\n",
    "\n",
    "# Tweak spacing to prevent clipping of tick-labels\n",
    "plt.subplots_adjust(bottom=0.15)\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0,np.max(freq)+10])\n",
    "\n",
    "\n",
    "#plt.savefig('histogram.png', dpi = 1000)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bokeh.charts import Bar, show, output_notebook\n",
    "from bokeh.layouts import row\n",
    "\n",
    "# best support is with data in a format that is table-like\n",
    "# data = {\n",
    "#     'sample': ['1st', '2nd', '1st', '2nd', '1st', '2nd'],\n",
    "#     'interpreter': ['python', 'python', 'pypy', 'pypy', 'jython', 'jython'],\n",
    "    \n",
    "# }\n",
    "from bokeh.charts.attributes import ColorAttr, CatAttr\n",
    "data = {\n",
    "    'freq': freq,\n",
    "    'topics': terms,\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "bar2 = Bar(data, values='freq', label=CatAttr(columns=['topics'], sort=False)\n",
    "           , title=\"Topic Frequency\", plot_width=900,legend=False)\n",
    "\n",
    "bar2.xaxis.major_label_orientation = \"vertical\"\n",
    "#p.yaxis.major_label_orientation = \"vertical\"\n",
    "\n",
    "output_notebook()\n",
    "#show(row(bar, bar2))\n",
    "show(bar2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_ids = []\n",
    "subreddit_ids = []\n",
    "subreddit_to_id = {}\n",
    "j= 0\n",
    "\n",
    "for i in input_data:\n",
    "\t\n",
    "    for sr in i.rstrip().split(\" \")[0:]: \n",
    "        if sr not in subreddit_to_id: \n",
    "            subreddit_to_id[sr] = len(subreddit_to_id)\n",
    "        user_ids.append(j)\n",
    "        subreddit_ids.append(subreddit_to_id[sr])\n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix \n",
    "\n",
    "rows = np.array(subreddit_ids)\n",
    "cols = np.array(user_ids)\n",
    "data = np.ones((len(user_ids),))\n",
    "num_rows = len(subreddit_to_id)\n",
    "num_cols = j\n",
    "\n",
    "# the code above exists to feed this call\n",
    "adj = csr_matrix( (data,(rows,cols)), shape=(num_rows, num_cols) )\n",
    "print adj.shape\n",
    "print \"\"\n",
    "\n",
    "# now we have our matrix, so let's gather up a bit of info about it\n",
    "users_per_subreddit = adj.sum(axis=1).A1\n",
    "subreddits = range(len(subreddit_to_id))\n",
    "for sr in subreddit_to_id:\n",
    "    subreddits[subreddit_to_id[sr]] = sr\n",
    "subreddits = np.array(subreddits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new=[]\n",
    "total=[]\n",
    "patent=[]\n",
    "total_patent=[]\n",
    "\n",
    "for i in input_data:\n",
    "    new= i.rstrip().split(\" \")[0:]\n",
    "    total.append(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "patents = range(len(subreddit_to_id))\n",
    "for sr in subreddit_to_id:\n",
    "\n",
    "    z =0\n",
    "    for j in total:\n",
    "        for w in total[z]: \n",
    "            if w == sr:\n",
    "            \tpatent.append(z)\n",
    "        z = z +1 \n",
    "    patents[subreddit_to_id[sr]] = patent\n",
    "    #total_patent.append(patent)\n",
    "    patent = []\n",
    "patents = np.array(patents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print subreddits[row_selector]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print patents[row_selector]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD \n",
    "from sklearn.preprocessing import normalize \n",
    "\n",
    "svd = TruncatedSVD(n_components=30)\n",
    "embedded_coords = normalize(svd.fit_transform(adj), norm='l1')\n",
    "print embedded_coords.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import bokeh.plotting as bp\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.models import HoverTool \n",
    "bp.output_notebook()\n",
    "row_selector = np.where(users_per_subreddit>1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from scipy.stats import rankdata\n",
    "embedded_ranks = np.array([rankdata(c) for c in embedded_coords.T]).T\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "n_clusters = 20\n",
    "km = KMeans(n_clusters)\n",
    "clusters = km.fit_predict(embedded_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bokeh.models import HoverTool\n",
    "\n",
    "colormap = np.array([\n",
    "    \"#1f77b4\", \"#aec7e8\", \"#ff7f0e\", \"#ffbb78\", \"#2ca02c\", \n",
    "    \"#98df8a\", \"#d62728\", \"#ff9896\", \"#9467bd\", \"#c5b0d5\", \n",
    "    \"#8c564b\", \"#c49c94\", \"#e377c2\", \"#f7b6d2\", \"#7f7f7f\", \n",
    "    \"#c7c7c7\", \"#bcbd22\", \"#dbdb8d\", \"#17becf\", \"#9edae5\"\n",
    "])\n",
    "\n",
    "# source =bp.ColumnDataSource({\"subreddit\": subreddits[row_selector],\n",
    "#                            \"patents\": patents[row_selector]}\n",
    "#                            )\n",
    "\n",
    "# hover = HoverTool(\n",
    "#   tooltips=\"\"\"\n",
    "#         <div>\n",
    "\n",
    "#             <div>\n",
    "#             <span style=\"font-size: 10px; font-weight: bold;\">@subreddit</span>   \n",
    "#             </div>\n",
    "#             <div>\n",
    "#             <span style=\"font-size: 5px;font-weight: bold;\">@patents</span>\n",
    "#             </div>\n",
    "#        </div>\n",
    "#         \"\"\"\n",
    "#     )\n",
    "\n",
    "# TOOLS= \"pan,wheel_zoom,box_zoom,crosshair,reset,previewsave\"\n",
    "\n",
    "# tbf = bp.figure(plot_width=900, plot_height=700, title=\"Subreddit Map by Interesting Dimensions\",\n",
    "#        x_axis_label = \"Dimension 0)\",\n",
    "#        y_axis_label = \"Dimension 1)\",\n",
    "#        tools=[hover,TOOLS],\n",
    "#        min_border=1)\n",
    "\n",
    "\n",
    "# tbf.scatter(\n",
    "#     x = embedded_coords[:,0][row_selector],\n",
    "#     y = embedded_coords[:,1][row_selector],\n",
    "#     color= colormap[clusters[row_selector]], \n",
    "#     radius= np.log2(users_per_subreddit[row_selector])/6000, \n",
    "#     source=source\n",
    "# )\n",
    "\n",
    "# bp.show(tbf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "xycoords = TSNE().fit_transform(embedded_coords[row_selector])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source =bp.ColumnDataSource({\"subreddit\": subreddits[row_selector],\n",
    "                           \"patents\": patents[row_selector]}\n",
    "                           )\n",
    "\n",
    "\n",
    "hover = HoverTool(\n",
    "tooltips=\"\"\"\n",
    "        <div>\n",
    "            <div>\n",
    "            <span style=\"font-size: 10px; font-weight: bold;\">@subreddit</span>\n",
    "            </div>\n",
    "            <div>\n",
    "            <span style=\"font-size: 5px;font-weight: bold;\">@patents</span>\n",
    "            </div>\n",
    "        </div>\n",
    "        \"\"\"   )\n",
    "\n",
    "TOOLS= \"pan,wheel_zoom,box_zoom,crosshair,reset,previewsave\"\n",
    "\n",
    "tbf = bp.figure(plot_width=900, plot_height=700, title=\"Subreddit Map by t-SNE\",\n",
    "       tools=[hover,TOOLS],\n",
    "       x_axis_type=None, y_axis_type=None, min_border=1)\n",
    "tbf.scatter(\n",
    "    x = embedded_coords[:,0],\n",
    "    y = embedded_coords[:,1],\n",
    "    color= colormap[clusters[row_selector]], \n",
    "    radius= np.log2(users_per_subreddit[row_selector])/6000, \n",
    "    source=source\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "bp.show(tbf)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
